---
title: "STAT/MATH 495: Problem Set 09"
author: "MERON GEDRAGO"
date: "2017-11-07"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

library(tidyverse)
library(lattice)
```



# Collaboration

Please indicate who you collaborated with on this assignment: 



# Question 1: Run k-means

```{r}
observations_1 <- read_csv("data/observations_1.csv")
observations_2 <- read_csv("data/observations_2.csv")

dim(observations_1)
dim(observations_2)
```


```{r}
cluster_center_list <- list()
mutated_observation_list <- list()
set.seed(1002)
for (i in 1:10){
  k <- 2
  k_means_results <- kmeans(observations_1, centers = k)
  clusters <- k_means_results$cluster
  cluster_centers <- k_means_results$centers
  
  # Add cluster results to observations. Note we convert to factor since cluster
  # ID's should be treated as categorical
  dataframe <- data.frame(x1 = observations_1[,1], x2 = observations_1[,2], cluster = as.factor(clusters))
  mutated_observation_list[[i]] <- dataframe

  # Add cluster ID's to cluster_centers
  cluster_centers <- cluster_centers %>% 
    as_tibble() %>% 
    mutate(cluster=as.factor(1:k))
  cluster_center_list[[i]] <- cluster_centers
}

plots <- list()

for (i in 1:10){
  plot <- ggplot(NULL, aes(x=x1, y=x2, col=cluster)) +
  geom_point(data=mutated_observation_list[[i]]) +
  geom_point(data=cluster_center_list[[i]], size=5)
  
  plots[[i]] <- plot
}

cowplot::plot_grid(plotlist = plots, nrow = 5, ncol = 2)
```



```{r}
cluster_center_list <- list()
mutated_observation_list <- list()
set.seed(1002)
for (i in 1:10){
  k <- 2
  k_means_results <- kmeans(observations_2, centers = k)
  clusters <- k_means_results$cluster
  cluster_centers <- k_means_results$centers
  
  # Add cluster results to observations. Note we convert to factor since cluster
  # ID's should be treated as categorical
  dataframe <- data.frame(x1 = observations_1[,1], x2 = observations_1[,2], cluster = as.factor(clusters))
  mutated_observation_list[[i]] <- dataframe

  # Add cluster ID's to cluster_centers
  cluster_centers <- cluster_centers %>% 
    as_tibble() %>% 
    mutate(cluster=as.factor(1:k))
  cluster_center_list[[i]] <- cluster_centers
}

plots <- list()

for (i in 1:10){
  plot <- ggplot(NULL, aes(x=x1, y=x2, col=cluster)) +
  geom_point(data=mutated_observation_list[[i]]) +
  geom_point(data=cluster_center_list[[i]], size=5)
  
  plots[[i]] <- plot
}

cowplot::plot_grid(plotlist = plots, nrow = 5, ncol = 2)
```


**Questions**:

1. Run KMC 10 times on `observations_1` and comment on the consistency of the
results.

The within cluster variation seems to be the same the the one with 
1. Speculate on the root cause of any consistency or inconsistency in the
results.
1. Run KMC 10 times on `observations_2` and comment on the consistency of the
results.
1. Speculate on the root cause of any consistency or inconsistency in the
results.

**Answers**:

1. The graph shows that the clusters and the centroid of the `observations_1` are constantly changing, thus KMX of `observations_1` is inconsistent. 

1.The result on the graph is caused by the 2b part of the KMC algorithm ( '2)b Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).'). Since the graph of the points are very spread out and distances between points is fairly the same for throughout the graph, the centroid keeps getting moved back and forward. 

1. The second graph shows that the each cluster seem to contain the roughly the same data points and the graph also shows the the centroid stays at the same place through the iteration , therefore the KMC of  `observations_2` is consistent. 
 
1. The result on the graph is caused by the 2b part of the KMC algorithm ( '2)b Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).'). There is a higher distance between points of the two clusters and closer distance between points within the cluster, therefore the centroid tends to stay in the same vicinity. 


# Bonus question: Code your own

Read ISLR page 388 Algorithm 10.1 and implement k-means clustering from scratch.
Don't worry about doing it for general $k$; keep it simple and do it for $k=2$
specifically. Apply it to `observations_2` from above.

```{r}
# Hint:
library(proxy)
#assign k=2 to the 
A <- data_frame(
  x1 = c(0, 0.5, 0.75, 1),
  x2 = c(0, 0.5, 0.75, 1)
)
B <- data_frame(
  x1 = c(1, 0),
  x2 = c(1, 0)
)
#calculating the distance between the points 
distance_matrix <- proxy::dist(x=A, y=B)
distance_matrix
#find the minium distance between the points   
apply(distance_matrix, 1, which.min)

#assign numbers from 1 to K , to each of the observations 
 




```











